{
  "('pearsonr', 'consistency', 'new', 'bertscore', 'precision')": 0.3798661812,
  "('pearsonr', 'consistency', 'new', 'bertscore', 'recall')": 0.1066469817,
  "('pearsonr', 'consistency', 'new', 'bertscore', 'f1')": 0.2398938649,
  "('pearsonr', 'consistency', 'new', 'bertscore-anyref-bart', 'precision')": 0.4006367928,
  "('pearsonr', 'consistency', 'new', 'bertscore-anyref-bart', 'recall')": 0.2659815426,
  "('pearsonr', 'consistency', 'new', 'bertscore-anyref-bart', 'f1')": 0.3490723543,
  "('pearsonr', 'consistency', 'new', 'bertscore-anyref-pegasus', 'precision')": -0.0552304542,
  "('pearsonr', 'consistency', 'new', 'bertscore-anyref-pegasus', 'recall')": -0.0187171697,
  "('pearsonr', 'consistency', 'new', 'bertscore-anyref-pegasus', 'f1')": -0.0415842222,
  "('pearsonr', 'consistency', 'new', 'rouge', 'rouge1')": 0.1264433976,
  "('pearsonr', 'consistency', 'new', 'rouge', 'rouge2')": 0.1752784613,
  "('pearsonr', 'consistency', 'new', 'rouge', 'rougeL')": 0.1606514212,
  "('pearsonr', 'consistency', 'new', 'rouge', 'rougeLsum')": 0.1606514212,
  "('pearsonr', 'consistency', 'new', 'rouge-anyref-bart', 'rouge1')": 0.3494452323,
  "('pearsonr', 'consistency', 'new', 'rouge-anyref-bart', 'rouge2')": 0.3571304303,
  "('pearsonr', 'consistency', 'new', 'rouge-anyref-bart', 'rougeL')": 0.3543151442,
  "('pearsonr', 'consistency', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.3543151442,
  "('pearsonr', 'consistency', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.1093218803,
  "('pearsonr', 'consistency', 'new', 'rouge-anyref-pegasus', 'rouge2')": -0.0752961121,
  "('pearsonr', 'consistency', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.1042763881,
  "('pearsonr', 'consistency', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.1042763881,
  "('pearsonr', 'consistency', 'new', 'bleurt', 'scores')": 0.3616115779,
  "('pearsonr', 'consistency', 'new', 'bleurt-anyref-bart', 'scores')": 0.4053199188,
  "('pearsonr', 'consistency', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.0541311609,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.1928473391,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.10898753,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.1902080009,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.0900346454,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.0504193139,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.0792121088,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.165622161,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.0723957793,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.1493099624,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.2030761345,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.1077837471,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.1989136883,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.1014647154,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.046132753,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.0886754638,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'meteor')": 0.1152538017,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'cider')": 0.0078069275,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 's3_pyr')": 0.0860228614,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 's3_resp')": 0.0770267588,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'mover_score')": 0.123532899,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.0027276597,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'bleu')": 0.0871730268,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.137943473,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.0636072988,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.1169289068,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'blanc')": 0.1527217801,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": -0.0890990116,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": -0.0092925338,
  "('pearsonr', 'consistency', 'PreCalc', 'PreCalc', 'supert')": 0.1622601214,
  "('pearsonr', 'relevance', 'new', 'bertscore', 'precision')": 0.5656262609,
  "('pearsonr', 'relevance', 'new', 'bertscore', 'recall')": 0.3391600614,
  "('pearsonr', 'relevance', 'new', 'bertscore', 'f1')": 0.4837595004,
  "('pearsonr', 'relevance', 'new', 'bertscore-anyref-bart', 'precision')": 0.5478113456,
  "('pearsonr', 'relevance', 'new', 'bertscore-anyref-bart', 'recall')": 0.5510484517,
  "('pearsonr', 'relevance', 'new', 'bertscore-anyref-bart', 'f1')": 0.5779624005,
  "('pearsonr', 'relevance', 'new', 'bertscore-anyref-pegasus', 'precision')": 0.1664597489,
  "('pearsonr', 'relevance', 'new', 'bertscore-anyref-pegasus', 'recall')": 0.112350239,
  "('pearsonr', 'relevance', 'new', 'bertscore-anyref-pegasus', 'f1')": 0.1522482099,
  "('pearsonr', 'relevance', 'new', 'rouge', 'rouge1')": 0.2339616553,
  "('pearsonr', 'relevance', 'new', 'rouge', 'rouge2')": 0.2672556748,
  "('pearsonr', 'relevance', 'new', 'rouge', 'rougeL')": 0.2662323237,
  "('pearsonr', 'relevance', 'new', 'rouge', 'rougeLsum')": 0.2662323237,
  "('pearsonr', 'relevance', 'new', 'rouge-anyref-bart', 'rouge1')": 0.5207149271,
  "('pearsonr', 'relevance', 'new', 'rouge-anyref-bart', 'rouge2')": 0.4809289862,
  "('pearsonr', 'relevance', 'new', 'rouge-anyref-bart', 'rougeL')": 0.5022990932,
  "('pearsonr', 'relevance', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.5022990932,
  "('pearsonr', 'relevance', 'new', 'rouge-anyref-pegasus', 'rouge1')": 0.0806816497,
  "('pearsonr', 'relevance', 'new', 'rouge-anyref-pegasus', 'rouge2')": 0.1136849152,
  "('pearsonr', 'relevance', 'new', 'rouge-anyref-pegasus', 'rougeL')": 0.0686528468,
  "('pearsonr', 'relevance', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": 0.0686528468,
  "('pearsonr', 'relevance', 'new', 'bleurt', 'scores')": 0.5471555737,
  "('pearsonr', 'relevance', 'new', 'bleurt-anyref-bart', 'scores')": 0.5709077265,
  "('pearsonr', 'relevance', 'new', 'bleurt-anyref-pegasus', 'scores')": 0.1483601538,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.3952308434,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.3743485779,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.4616943085,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.3276489806,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.2973357373,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.3342643611,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.3945629685,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.3492899182,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.4402965158,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.3942776014,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.3770652543,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.4637232353,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.3631782531,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.3394488358,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.3839116293,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'meteor')": 0.3650724764,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'cider')": 0.0300168335,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 's3_pyr')": 0.3591783447,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 's3_resp')": 0.3182991777,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'mover_score')": 0.3093894778,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.2014758225,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'bleu')": 0.3022943936,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.326301237,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.2890328972,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.3463690254,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'blanc')": 0.260111342,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.094107616,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.1875533817,
  "('pearsonr', 'relevance', 'PreCalc', 'PreCalc', 'supert')": 0.4128488176,
  "('pearsonr', 'coherence', 'new', 'bertscore', 'precision')": 0.5285340453,
  "('pearsonr', 'coherence', 'new', 'bertscore', 'recall')": 0.2724973195,
  "('pearsonr', 'coherence', 'new', 'bertscore', 'f1')": 0.4206328943,
  "('pearsonr', 'coherence', 'new', 'bertscore-anyref-bart', 'precision')": 0.4427493434,
  "('pearsonr', 'coherence', 'new', 'bertscore-anyref-bart', 'recall')": 0.4662969444,
  "('pearsonr', 'coherence', 'new', 'bertscore-anyref-bart', 'f1')": 0.4780276543,
  "('pearsonr', 'coherence', 'new', 'bertscore-anyref-pegasus', 'precision')": 0.0175361611,
  "('pearsonr', 'coherence', 'new', 'bertscore-anyref-pegasus', 'recall')": 0.0242478547,
  "('pearsonr', 'coherence', 'new', 'bertscore-anyref-pegasus', 'f1')": 0.0218909324,
  "('pearsonr', 'coherence', 'new', 'rouge', 'rouge1')": 0.1876589393,
  "('pearsonr', 'coherence', 'new', 'rouge', 'rouge2')": 0.2326290018,
  "('pearsonr', 'coherence', 'new', 'rouge', 'rougeL')": 0.2350240149,
  "('pearsonr', 'coherence', 'new', 'rouge', 'rougeLsum')": 0.2350240149,
  "('pearsonr', 'coherence', 'new', 'rouge-anyref-bart', 'rouge1')": 0.4217719631,
  "('pearsonr', 'coherence', 'new', 'rouge-anyref-bart', 'rouge2')": 0.4062559907,
  "('pearsonr', 'coherence', 'new', 'rouge-anyref-bart', 'rougeL')": 0.4247348244,
  "('pearsonr', 'coherence', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.4247348244,
  "('pearsonr', 'coherence', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.0542155063,
  "('pearsonr', 'coherence', 'new', 'rouge-anyref-pegasus', 'rouge2')": 0.0077207282,
  "('pearsonr', 'coherence', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.0505308073,
  "('pearsonr', 'coherence', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.0505308073,
  "('pearsonr', 'coherence', 'new', 'bleurt', 'scores')": 0.401291559,
  "('pearsonr', 'coherence', 'new', 'bleurt-anyref-bart', 'scores')": 0.4384137779,
  "('pearsonr', 'coherence', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.0095032905,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.3048027386,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.3326513894,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.3702920878,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.2278147457,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.2268289433,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.2392526981,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.2832601399,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.2879109866,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.3291801202,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.301716385,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.3312166396,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.3680082165,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.2340597734,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.2466198291,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.2580962864,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'meteor')": 0.2661959991,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'cider')": -0.0730350483,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 's3_pyr')": 0.2769539176,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 's3_resp')": 0.2485990889,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'mover_score')": 0.1818215858,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.1417483741,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'bleu')": 0.1638741634,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.2058824925,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.2944893795,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.2755968345,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'blanc')": 0.2103912794,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.0299313822,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.1182168612,
  "('pearsonr', 'coherence', 'PreCalc', 'PreCalc', 'supert')": 0.3238257805,
  "('pearsonr', 'fluency', 'new', 'bertscore', 'precision')": 0.2782301897,
  "('pearsonr', 'fluency', 'new', 'bertscore', 'recall')": 0.2986738738,
  "('pearsonr', 'fluency', 'new', 'bertscore', 'f1')": 0.3284847062,
  "('pearsonr', 'fluency', 'new', 'bertscore-anyref-bart', 'precision')": 0.2536467741,
  "('pearsonr', 'fluency', 'new', 'bertscore-anyref-bart', 'recall')": 0.2285397535,
  "('pearsonr', 'fluency', 'new', 'bertscore-anyref-bart', 'f1')": 0.2539063182,
  "('pearsonr', 'fluency', 'new', 'bertscore-anyref-pegasus', 'precision')": -0.0302471118,
  "('pearsonr', 'fluency', 'new', 'bertscore-anyref-pegasus', 'recall')": -0.0456748444,
  "('pearsonr', 'fluency', 'new', 'bertscore-anyref-pegasus', 'f1')": -0.040373134,
  "('pearsonr', 'fluency', 'new', 'rouge', 'rouge1')": 0.2265707731,
  "('pearsonr', 'fluency', 'new', 'rouge', 'rouge2')": 0.237083422,
  "('pearsonr', 'fluency', 'new', 'rouge', 'rougeL')": 0.2222733989,
  "('pearsonr', 'fluency', 'new', 'rouge', 'rougeLsum')": 0.2222733989,
  "('pearsonr', 'fluency', 'new', 'rouge-anyref-bart', 'rouge1')": 0.2636537643,
  "('pearsonr', 'fluency', 'new', 'rouge-anyref-bart', 'rouge2')": 0.2014132772,
  "('pearsonr', 'fluency', 'new', 'rouge-anyref-bart', 'rougeL')": 0.2076155523,
  "('pearsonr', 'fluency', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.2076155523,
  "('pearsonr', 'fluency', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.0553571072,
  "('pearsonr', 'fluency', 'new', 'rouge-anyref-pegasus', 'rouge2')": -0.0173182325,
  "('pearsonr', 'fluency', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.0514082859,
  "('pearsonr', 'fluency', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.0514082859,
  "('pearsonr', 'fluency', 'new', 'bleurt', 'scores')": 0.3284155548,
  "('pearsonr', 'fluency', 'new', 'bleurt-anyref-bart', 'scores')": 0.2356271893,
  "('pearsonr', 'fluency', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.0157252469,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.1192631956,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.0583407253,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.1116405585,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.0922183945,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.0519174599,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.0802845221,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.137395204,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.0734994921,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.1261982409,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.1272196026,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.0518449744,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.1159383725,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.1233496196,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.0764509577,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.1127036235,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'meteor')": 0.0952834881,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'cider')": 0.0994879524,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 's3_pyr')": 0.0689644068,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 's3_resp')": 0.0524669427,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'mover_score')": 0.143413337,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.0229436054,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'bleu')": 0.1012940427,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.2116995809,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.1123291499,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.1866897852,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'blanc')": 0.1940961232,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.2498227398,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.2569688642,
  "('pearsonr', 'fluency', 'PreCalc', 'PreCalc', 'supert')": 0.1849887562,
  "('kendalltau', 'consistency', 'new', 'bertscore', 'precision')": 0.245983156,
  "('kendalltau', 'consistency', 'new', 'bertscore', 'recall')": 0.0550708558,
  "('kendalltau', 'consistency', 'new', 'bertscore', 'f1')": 0.1607253126,
  "('kendalltau', 'consistency', 'new', 'bertscore-anyref-bart', 'precision')": 0.2724987533,
  "('kendalltau', 'consistency', 'new', 'bertscore-anyref-bart', 'recall')": 0.2092692521,
  "('kendalltau', 'consistency', 'new', 'bertscore-anyref-bart', 'f1')": 0.2569973272,
  "('kendalltau', 'consistency', 'new', 'bertscore-anyref-pegasus', 'precision')": -0.0089745098,
  "('kendalltau', 'consistency', 'new', 'bertscore-anyref-pegasus', 'recall')": 0.0069348485,
  "('kendalltau', 'consistency', 'new', 'bertscore-anyref-pegasus', 'f1')": 0.0114221034,
  "('kendalltau', 'consistency', 'new', 'rouge', 'rouge1')": 0.0779150627,
  "('kendalltau', 'consistency', 'new', 'rouge', 'rouge2')": 0.1231955441,
  "('kendalltau', 'consistency', 'new', 'rouge', 'rougeL')": 0.1097337794,
  "('kendalltau', 'consistency', 'new', 'rouge', 'rougeLsum')": 0.1097337794,
  "('kendalltau', 'consistency', 'new', 'rouge-anyref-bart', 'rouge1')": 0.2149803039,
  "('kendalltau', 'consistency', 'new', 'rouge-anyref-bart', 'rouge2')": 0.2133485748,
  "('kendalltau', 'consistency', 'new', 'rouge-anyref-bart', 'rougeL')": 0.2149803039,
  "('kendalltau', 'consistency', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.2149803039,
  "('kendalltau', 'consistency', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.0414932344,
  "('kendalltau', 'consistency', 'new', 'rouge-anyref-pegasus', 'rouge2')": -0.0521088512,
  "('kendalltau', 'consistency', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.0533484443,
  "('kendalltau', 'consistency', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.0533484443,
  "('kendalltau', 'consistency', 'new', 'bleurt', 'scores')": 0.2288500009,
  "('kendalltau', 'consistency', 'new', 'bleurt-anyref-bart', 'scores')": 0.2769860082,
  "('kendalltau', 'consistency', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.033858378,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.1378811057,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.0848499112,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.1456318187,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.0779150627,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.0501756686,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.0758754014,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.1150368988,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.0669008915,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.1223796796,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.1484873446,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.0917847597,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.1505270059,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.0807705885,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.0416090911,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.07383574,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'meteor')": 0.1044306599,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'cider')": -0.0473201428,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 's3_pyr')": 0.0856657757,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 's3_resp')": 0.0742436723,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'mover_score')": 0.1227876119,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.0057110517,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'bleu')": 0.0620057043,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.1085099826,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.0509915332,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.0901530306,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'blanc')": 0.0620057043,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": -0.0832181821,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": -0.0004079323,
  "('kendalltau', 'consistency', 'PreCalc', 'PreCalc', 'supert')": 0.0775071304,
  "('kendalltau', 'relevance', 'new', 'bertscore', 'precision')": 0.4142705203,
  "('kendalltau', 'relevance', 'new', 'bertscore', 'recall')": 0.2269975454,
  "('kendalltau', 'relevance', 'new', 'bertscore', 'f1')": 0.3307678518,
  "('kendalltau', 'relevance', 'new', 'bertscore-anyref-bart', 'precision')": 0.3802208885,
  "('kendalltau', 'relevance', 'new', 'bertscore-anyref-bart', 'recall')": 0.3822476523,
  "('kendalltau', 'relevance', 'new', 'bertscore-anyref-bart', 'f1')": 0.3960296461,
  "('kendalltau', 'relevance', 'new', 'bertscore-anyref-pegasus', 'precision')": 0.1430895241,
  "('kendalltau', 'relevance', 'new', 'bertscore-anyref-pegasus', 'recall')": 0.0887722544,
  "('kendalltau', 'relevance', 'new', 'bertscore-anyref-pegasus', 'f1')": 0.128091472,
  "('kendalltau', 'relevance', 'new', 'rouge', 'rouge1')": 0.1621411038,
  "('kendalltau', 'relevance', 'new', 'rouge', 'rouge2')": 0.188894386,
  "('kendalltau', 'relevance', 'new', 'rouge', 'rougeL')": 0.200649616,
  "('kendalltau', 'relevance', 'new', 'rouge', 'rougeLsum')": 0.200649616,
  "('kendalltau', 'relevance', 'new', 'rouge-anyref-bart', 'rouge1')": 0.3275250298,
  "('kendalltau', 'relevance', 'new', 'rouge-anyref-bart', 'rouge2')": 0.315364447,
  "('kendalltau', 'relevance', 'new', 'rouge-anyref-bart', 'rougeL')": 0.3190126218,
  "('kendalltau', 'relevance', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.3190126218,
  "('kendalltau', 'relevance', 'new', 'rouge-anyref-pegasus', 'rouge1')": 0.060729194,
  "('kendalltau', 'relevance', 'new', 'rouge-anyref-pegasus', 'rouge2')": 0.0571643999,
  "('kendalltau', 'relevance', 'new', 'rouge-anyref-pegasus', 'rougeL')": 0.0481365183,
  "('kendalltau', 'relevance', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": 0.0481365183,
  "('kendalltau', 'relevance', 'new', 'bleurt', 'scores')": 0.3636014254,
  "('kendalltau', 'relevance', 'new', 'bleurt-anyref-bart', 'scores')": 0.3834637106,
  "('kendalltau', 'relevance', 'new', 'bleurt-anyref-pegasus', 'scores')": 0.0972846623,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.2938807507,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.2424009502,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.3344160267,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.2407795392,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.2055138491,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.2379420699,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.2983396311,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.234293895,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.328741088,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.3003663949,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.2452384196,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.3368481432,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.2703702907,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.2322671313,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.2772612876,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'meteor')": 0.244427714,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'cider')": -0.0060802914,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 's3_pyr')": 0.2338885423,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 's3_resp')": 0.2156476681,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'mover_score')": 0.2282136037,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.13660388,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'bleu')": 0.2298350147,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.2492919472,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.1901104443,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.2549668858,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'blanc')": 0.1852462111,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.0587761501,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.1122827144,
  "('kendalltau', 'relevance', 'PreCalc', 'PreCalc', 'supert')": 0.29671822,
  "('kendalltau', 'coherence', 'new', 'bertscore', 'precision')": 0.3441722583,
  "('kendalltau', 'coherence', 'new', 'bertscore', 'recall')": 0.1817083671,
  "('kendalltau', 'coherence', 'new', 'bertscore', 'f1')": 0.2886671384,
  "('kendalltau', 'coherence', 'new', 'bertscore-anyref-bart', 'precision')": 0.3113553626,
  "('kendalltau', 'coherence', 'new', 'bertscore-anyref-bart', 'recall')": 0.3295869713,
  "('kendalltau', 'coherence', 'new', 'bertscore-anyref-bart', 'f1')": 0.3372847617,
  "('kendalltau', 'coherence', 'new', 'bertscore-anyref-pegasus', 'precision')": 0.0326143223,
  "('kendalltau', 'coherence', 'new', 'bertscore-anyref-pegasus', 'recall')": 0.0322091754,
  "('kendalltau', 'coherence', 'new', 'bertscore-anyref-pegasus', 'f1')": 0.0220805039,
  "('kendalltau', 'coherence', 'new', 'rouge', 'rouge1')": 0.120936338,
  "('kendalltau', 'coherence', 'new', 'rouge', 'rouge2')": 0.1565892617,
  "('kendalltau', 'coherence', 'new', 'rouge', 'rougeL')": 0.1659076395,
  "('kendalltau', 'coherence', 'new', 'rouge', 'rougeLsum')": 0.1659076395,
  "('kendalltau', 'coherence', 'new', 'rouge-anyref-bart', 'rouge1')": 0.2919083133,
  "('kendalltau', 'coherence', 'new', 'rouge-anyref-bart', 'rouge2')": 0.2858311104,
  "('kendalltau', 'coherence', 'new', 'rouge-anyref-bart', 'rougeL')": 0.2878568447,
  "('kendalltau', 'coherence', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.2878568447,
  "('kendalltau', 'coherence', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.0357287927,
  "('kendalltau', 'coherence', 'new', 'rouge-anyref-pegasus', 'rouge2')": -0.0287746938,
  "('kendalltau', 'coherence', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.0414129188,
  "('kendalltau', 'coherence', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.0414129188,
  "('kendalltau', 'coherence', 'new', 'bleurt', 'scores')": 0.2793487606,
  "('kendalltau', 'coherence', 'new', 'bleurt-anyref-bart', 'scores')": 0.3064936003,
  "('kendalltau', 'coherence', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.0127621261,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.2193870252,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.2084480599,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.2566605364,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.1432194153,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.1367370656,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.1415988279,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.1902164512,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.184949542,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.2254642281,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.2246539344,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.2137149691,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.2546348021,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.15294294,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.1505120588,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.1719848424,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'meteor')": 0.1626664646,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'cider')": -0.0873091485,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 's3_pyr')": 0.1695539613,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 's3_resp')": 0.1505120588,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'mover_score')": 0.1367370656,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.0808267987,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'bleu')": 0.1164797225,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.1464605902,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.1829238077,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.1934576261,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'blanc')": 0.1363319187,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.0010128672,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.0634054837,
  "('kendalltau', 'coherence', 'PreCalc', 'PreCalc', 'supert')": 0.2586862707,
  "('kendalltau', 'fluency', 'new', 'bertscore', 'precision')": 0.1658401188,
  "('kendalltau', 'fluency', 'new', 'bertscore', 'recall')": 0.1784561769,
  "('kendalltau', 'fluency', 'new', 'bertscore', 'f1')": 0.2053161716,
  "('kendalltau', 'fluency', 'new', 'bertscore-anyref-bart', 'precision')": 0.1304337622,
  "('kendalltau', 'fluency', 'new', 'bertscore-anyref-bart', 'recall')": 0.1011319498,
  "('kendalltau', 'fluency', 'new', 'bertscore-anyref-bart', 'f1')": 0.1247361875,
  "('kendalltau', 'fluency', 'new', 'bertscore-anyref-pegasus', 'precision')": -0.023400753,
  "('kendalltau', 'fluency', 'new', 'bertscore-anyref-pegasus', 'recall')": -0.0474119603,
  "('kendalltau', 'fluency', 'new', 'bertscore-anyref-pegasus', 'f1')": -0.0299122668,
  "('kendalltau', 'fluency', 'new', 'rouge', 'rouge1')": 0.1332825495,
  "('kendalltau', 'fluency', 'new', 'rouge', 'rouge2')": 0.1430498203,
  "('kendalltau', 'fluency', 'new', 'rouge', 'rougeL')": 0.142235881,
  "('kendalltau', 'fluency', 'new', 'rouge', 'rougeLsum')": 0.142235881,
  "('kendalltau', 'fluency', 'new', 'rouge-anyref-bart', 'rouge1')": 0.1052016459,
  "('kendalltau', 'fluency', 'new', 'rouge-anyref-bart', 'rouge2')": 0.0929925574,
  "('kendalltau', 'fluency', 'new', 'rouge-anyref-bart', 'rougeL')": 0.0917716486,
  "('kendalltau', 'fluency', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.0917716486,
  "('kendalltau', 'fluency', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.0354817009,
  "('kendalltau', 'fluency', 'new', 'rouge-anyref-pegasus', 'rouge2')": -0.0309835865,
  "('kendalltau', 'fluency', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.0415992355,
  "('kendalltau', 'fluency', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.0415992355,
  "('kendalltau', 'fluency', 'new', 'bleurt', 'scores')": 0.1426428507,
  "('kendalltau', 'fluency', 'new', 'bleurt-anyref-bart', 'scores')": 0.1247361875,
  "('kendalltau', 'fluency', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.0168892391,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.0665395323,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.0030522721,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.0225868137,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.0323540845,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.0075289379,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.0115986341,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.0669465019,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.0168892391,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.0307262061,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.0685743804,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.0034592417,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.0290983276,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.0628768058,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.0343889326,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.0437492338,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'meteor')": 0.0331680238,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'cider')": 0.0555513527,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 's3_pyr')": 0.0038662114,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 's3_resp')": 0.0099707556,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'mover_score')": 0.1426428507,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.053109535,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'bleu')": 0.0172962087,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.1279919445,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.0356098415,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.0848531651,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'blanc')": 0.1035737675,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.1418289114,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.1650261796,
  "('kendalltau', 'fluency', 'PreCalc', 'PreCalc', 'supert')": 0.096655284,
  "('spearmanr', 'consistency', 'new', 'bertscore', 'precision')": 0.3536231422,
  "('spearmanr', 'consistency', 'new', 'bertscore', 'recall')": 0.0737407782,
  "('spearmanr', 'consistency', 'new', 'bertscore', 'f1')": 0.2292670013,
  "('spearmanr', 'consistency', 'new', 'bertscore-anyref-bart', 'precision')": 0.3858318269,
  "('spearmanr', 'consistency', 'new', 'bertscore-anyref-bart', 'recall')": 0.3021792988,
  "('spearmanr', 'consistency', 'new', 'bertscore-anyref-bart', 'f1')": 0.3620460284,
  "('spearmanr', 'consistency', 'new', 'bertscore-anyref-pegasus', 'precision')": -0.0165275879,
  "('spearmanr', 'consistency', 'new', 'bertscore-anyref-pegasus', 'recall')": 0.009017231,
  "('spearmanr', 'consistency', 'new', 'bertscore-anyref-pegasus', 'f1')": 0.0106681888,
  "('spearmanr', 'consistency', 'new', 'rouge', 'rouge1')": 0.1136639382,
  "('spearmanr', 'consistency', 'new', 'rouge', 'rouge2')": 0.1813832232,
  "('spearmanr', 'consistency', 'new', 'rouge', 'rougeL')": 0.162526284,
  "('spearmanr', 'consistency', 'new', 'rouge', 'rougeLsum')": 0.162526284,
  "('spearmanr', 'consistency', 'new', 'rouge-anyref-bart', 'rouge1')": 0.3092574049,
  "('spearmanr', 'consistency', 'new', 'rouge-anyref-bart', 'rouge2')": 0.3124752717,
  "('spearmanr', 'consistency', 'new', 'rouge-anyref-bart', 'rougeL')": 0.3228372828,
  "('spearmanr', 'consistency', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.3228372828,
  "('spearmanr', 'consistency', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.0712973339,
  "('spearmanr', 'consistency', 'new', 'rouge-anyref-pegasus', 'rouge2')": -0.076212964,
  "('spearmanr', 'consistency', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.0778842612,
  "('spearmanr', 'consistency', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.0778842612,
  "('spearmanr', 'consistency', 'new', 'bleurt', 'scores')": 0.3268776267,
  "('spearmanr', 'consistency', 'new', 'bleurt-anyref-bart', 'scores')": 0.3904785226,
  "('spearmanr', 'consistency', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.0482619975,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.2061475894,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.1221648697,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.2077745332,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.1082487968,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.0702707651,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.1061295674,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.1770006808,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.0901783138,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.1749655002,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.2271297614,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.1319325361,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.2153689388,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.1215705249,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.0620760112,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.1150147218,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'meteor')": 0.1464669677,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'cider')": -0.0587620888,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 's3_pyr')": 0.1190190448,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 's3_resp')": 0.1034700245,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'mover_score')": 0.1808489132,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.0106501783,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'bleu')": 0.0891577218,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.162352183,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.0743771473,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.1307618569,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'blanc')": 0.0930299681,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": -0.1261992101,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": -0.0061235524,
  "('spearmanr', 'consistency', 'PreCalc', 'PreCalc', 'supert')": 0.1000780568,
  "('spearmanr', 'relevance', 'new', 'bertscore', 'precision')": 0.575944168,
  "('spearmanr', 'relevance', 'new', 'bertscore', 'recall')": 0.3362559965,
  "('spearmanr', 'relevance', 'new', 'bertscore', 'f1')": 0.4865899732,
  "('spearmanr', 'relevance', 'new', 'bertscore-anyref-bart', 'precision')": 0.5549454221,
  "('spearmanr', 'relevance', 'new', 'bertscore-anyref-bart', 'recall')": 0.5450611882,
  "('spearmanr', 'relevance', 'new', 'bertscore-anyref-bart', 'f1')": 0.5736816566,
  "('spearmanr', 'relevance', 'new', 'bertscore-anyref-pegasus', 'precision')": 0.2048623005,
  "('spearmanr', 'relevance', 'new', 'bertscore-anyref-pegasus', 'recall')": 0.1232618582,
  "('spearmanr', 'relevance', 'new', 'bertscore-anyref-pegasus', 'f1')": 0.1721609097,
  "('spearmanr', 'relevance', 'new', 'rouge', 'rouge1')": 0.239478124,
  "('spearmanr', 'relevance', 'new', 'rouge', 'rouge2')": 0.2720234795,
  "('spearmanr', 'relevance', 'new', 'rouge', 'rougeL')": 0.2839421732,
  "('spearmanr', 'relevance', 'new', 'rouge', 'rougeLsum')": 0.2839421732,
  "('spearmanr', 'relevance', 'new', 'rouge-anyref-bart', 'rouge1')": 0.4755534789,
  "('spearmanr', 'relevance', 'new', 'rouge-anyref-bart', 'rouge2')": 0.4560310666,
  "('spearmanr', 'relevance', 'new', 'rouge-anyref-bart', 'rougeL')": 0.4677517156,
  "('spearmanr', 'relevance', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.4677517156,
  "('spearmanr', 'relevance', 'new', 'rouge-anyref-pegasus', 'rouge1')": 0.0924124106,
  "('spearmanr', 'relevance', 'new', 'rouge-anyref-pegasus', 'rouge2')": 0.0896071455,
  "('spearmanr', 'relevance', 'new', 'rouge-anyref-pegasus', 'rougeL')": 0.0726286157,
  "('spearmanr', 'relevance', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": 0.0726286157,
  "('spearmanr', 'relevance', 'new', 'bleurt', 'scores')": 0.5357590858,
  "('spearmanr', 'relevance', 'new', 'bleurt-anyref-bart', 'scores')": 0.5559116404,
  "('spearmanr', 'relevance', 'new', 'bleurt-anyref-pegasus', 'scores')": 0.1364828462,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.4400134465,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.3574647899,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.4976324689,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.3615157054,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.308475718,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.3549862297,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.4444724543,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.3467163607,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.4929334069,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.4492795407,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.3637362073,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.5020914767,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.3979559412,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.3360579518,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.4087523813,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'meteor')": 0.3621578505,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'cider')": -0.0024185466,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 's3_pyr')": 0.3448379361,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 's3_resp')": 0.3226269162,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'mover_score')": 0.3586230517,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.1991250038,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'bleu')": 0.3457681463,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.3731223286,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.2884131837,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.386787417,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'blanc')": 0.2642577243,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.0886580374,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.1650613052,
  "('spearmanr', 'relevance', 'PreCalc', 'PreCalc', 'supert')": 0.417724409,
  "('spearmanr', 'coherence', 'new', 'bertscore', 'precision')": 0.4906441685,
  "('spearmanr', 'coherence', 'new', 'bertscore', 'recall')": 0.2767322408,
  "('spearmanr', 'coherence', 'new', 'bertscore', 'f1')": 0.4133550195,
  "('spearmanr', 'coherence', 'new', 'bertscore-anyref-bart', 'precision')": 0.4498961817,
  "('spearmanr', 'coherence', 'new', 'bertscore-anyref-bart', 'recall')": 0.4694900222,
  "('spearmanr', 'coherence', 'new', 'bertscore-anyref-bart', 'f1')": 0.4816003959,
  "('spearmanr', 'coherence', 'new', 'bertscore-anyref-pegasus', 'precision')": 0.0473912889,
  "('spearmanr', 'coherence', 'new', 'bertscore-anyref-pegasus', 'recall')": 0.0402738939,
  "('spearmanr', 'coherence', 'new', 'bertscore-anyref-pegasus', 'f1')": 0.0330004682,
  "('spearmanr', 'coherence', 'new', 'rouge', 'rouge1')": 0.1788110478,
  "('spearmanr', 'coherence', 'new', 'rouge', 'rouge2')": 0.2259802932,
  "('spearmanr', 'coherence', 'new', 'rouge', 'rougeL')": 0.237550561,
  "('spearmanr', 'coherence', 'new', 'rouge', 'rougeLsum')": 0.237550561,
  "('spearmanr', 'coherence', 'new', 'rouge-anyref-bart', 'rouge1')": 0.4196202475,
  "('spearmanr', 'coherence', 'new', 'rouge-anyref-bart', 'rouge2')": 0.4161755723,
  "('spearmanr', 'coherence', 'new', 'rouge-anyref-bart', 'rougeL')": 0.4220087157,
  "('spearmanr', 'coherence', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.4220087157,
  "('spearmanr', 'coherence', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.0425014706,
  "('spearmanr', 'coherence', 'new', 'rouge-anyref-pegasus', 'rouge2')": -0.0328043229,
  "('spearmanr', 'coherence', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.0575129349,
  "('spearmanr', 'coherence', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.0575129349,
  "('spearmanr', 'coherence', 'new', 'bleurt', 'scores')": 0.4071317997,
  "('spearmanr', 'coherence', 'new', 'bleurt-anyref-bart', 'scores')": 0.4417165785,
  "('spearmanr', 'coherence', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.0254209826,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.3272501425,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.3076803068,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.3796584148,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.2157122806,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.2126696842,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.2208252828,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.2919632261,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.2723933904,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.3363299222,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.3395525539,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.3139155289,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.3839732605,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.2381266739,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.2331516988,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.2561602086,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'meteor')": 0.2536457157,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'cider')": -0.1195134252,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 's3_pyr')": 0.2542398322,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 's3_resp')": 0.2348440305,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'mover_score')": 0.2130657619,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.1128461184,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'bleu')": 0.1721017327,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.215622263,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.2770983125,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.2899888391,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'blanc')": 0.2000852176,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.0059351633,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.0952146625,
  "('spearmanr', 'coherence', 'PreCalc', 'PreCalc', 'supert')": 0.356445865,
  "('spearmanr', 'fluency', 'new', 'bertscore', 'precision')": 0.2388684254,
  "('spearmanr', 'fluency', 'new', 'bertscore', 'recall')": 0.2696130937,
  "('spearmanr', 'fluency', 'new', 'bertscore', 'f1')": 0.3097756426,
  "('spearmanr', 'fluency', 'new', 'bertscore-anyref-bart', 'precision')": 0.2008787718,
  "('spearmanr', 'fluency', 'new', 'bertscore-anyref-bart', 'recall')": 0.1451037898,
  "('spearmanr', 'fluency', 'new', 'bertscore-anyref-bart', 'f1')": 0.1900142951,
  "('spearmanr', 'fluency', 'new', 'bertscore-anyref-pegasus', 'precision')": -0.0342861276,
  "('spearmanr', 'fluency', 'new', 'bertscore-anyref-pegasus', 'recall')": -0.0672036912,
  "('spearmanr', 'fluency', 'new', 'bertscore-anyref-pegasus', 'f1')": -0.0420053082,
  "('spearmanr', 'fluency', 'new', 'rouge', 'rouge1')": 0.1931775986,
  "('spearmanr', 'fluency', 'new', 'rouge', 'rouge2')": 0.2091321726,
  "('spearmanr', 'fluency', 'new', 'rouge', 'rougeL')": 0.2041441173,
  "('spearmanr', 'fluency', 'new', 'rouge', 'rougeLsum')": 0.2041441173,
  "('spearmanr', 'fluency', 'new', 'rouge-anyref-bart', 'rouge1')": 0.1621027942,
  "('spearmanr', 'fluency', 'new', 'rouge-anyref-bart', 'rouge2')": 0.1349295976,
  "('spearmanr', 'fluency', 'new', 'rouge-anyref-bart', 'rougeL')": 0.1334950064,
  "('spearmanr', 'fluency', 'new', 'rouge-anyref-bart', 'rougeLsum')": 0.1334950064,
  "('spearmanr', 'fluency', 'new', 'rouge-anyref-pegasus', 'rouge1')": -0.0527045692,
  "('spearmanr', 'fluency', 'new', 'rouge-anyref-pegasus', 'rouge2')": -0.0508302519,
  "('spearmanr', 'fluency', 'new', 'rouge-anyref-pegasus', 'rougeL')": -0.0649876935,
  "('spearmanr', 'fluency', 'new', 'rouge-anyref-pegasus', 'rougeLsum')": -0.0649876935,
  "('spearmanr', 'fluency', 'new', 'bleurt', 'scores')": 0.212193434,
  "('spearmanr', 'fluency', 'new', 'bleurt-anyref-bart', 'scores')": 0.1774931358,
  "('spearmanr', 'fluency', 'new', 'bleurt-anyref-pegasus', 'scores')": -0.030588604,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.1027083208,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.0055762977,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.0399704698,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.0591123572,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.0105343407,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.0254444844,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.1020180364,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.0297842726,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.0593224437,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_p')": 0.1084286779,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_r')": 0.0107264198,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_1_f')": 0.0435779562,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_p')": 0.0972640775,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_r')": 0.0608110571,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'rouge_we_2_f')": 0.0731461398,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'meteor')": 0.059034325,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'cider')": 0.0774619181,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 's3_pyr')": 0.0239198561,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 's3_resp')": 0.0190218379,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'mover_score')": 0.2030816795,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'sentence_movers_glove_sms')": 0.070385002,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'bleu')": 0.0299763517,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_precision')": 0.192967512,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_recall')": 0.059376466,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'bert_score_f1')": 0.1297254532,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'blanc')": 0.1429669093,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'summaqa_avg_prob')": 0.2112690531,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'summaqa_avg_fscore')": 0.2447028295,
  "('spearmanr', 'fluency', 'PreCalc', 'PreCalc', 'supert')": 0.1359740279
}