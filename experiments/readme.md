# Getting Source data files
* Newsroom: You will need two files to begin
  - `newsroom-human-eval.csv`: The human evaluation result, including documents and system summaries but no reference summaries. To get the first file, simply: 
    ```shell
    wget https://github.com/lil-lab/newsroom/raw/master/humaneval/newsroom-human-eval.csv
    ```
  - `test.jsonl`: The test split of Newsroom, containing reference summaries. No automatic script. You will have to fill out a web form [here](https://lil.nlp.cornell.edu/newsroom/download/index.html) and then follow the link in your email to download. `test.jsonl` is in the downloaded tar ball. 
* Realsumm: 
  ```shell 
   wget -O src.txt "https://drive.google.com/uc?export=download&id=1z1_i3cCQOd-1PWfaoFwO34YgCvdJemH7"

   wget -O abs.pkl "https://github.com/neulab/REALSumm/blob/master/scores_dicts/abs.pkl?raw=true"

   wget -O ext.pkl "https://github.com/neulab/REALSumm/blob/master/scores_dicts/ext.pkl?raw=true"
  ```
* TAC: We assume that you have fully recursively extracted the two files. 
  - [`GuidedSumm2010_eval.tgz`](https://tac.nist.gov/protected/past-aquaint-aquaint2/2010/GuidedSumm2010_eval.tgz
) Downloadable from web, containing human evaluation results and system summaries. 
  - `TAC2010_Summarization_Documents.tgz` Emailed by NIST, containing the documents for which summaries are generated and rated. 
  Both files require you to apply to NIST for access. 


# GPU

There are memory leaks when using GPU. 
Distable GPU: 
```shell
export CUDA_VISIBLE_DEVICES=''
```


# File structures
* `env.py`: configurations of experimental settings
* `eval.py`: scoring summaries using automated metrics and computing their correlations with human scores
* `bao_newsroom.py`: Run experiments on Newsroom dataset
* `corr.py`: may no longer needed. kept for reference. 
* `format.py`: 
* `table.py`: 


# Key `Pandas.DataFrame`s in `eval.py`
We use the same local variable names for key DataFrames across functions in `eval.py` to be consistent. 
Please read more details of such variables in the docstrings/comments inside `eval.py` 

1. `dataset_df`: A summary human evaluation dataset represented as a `pandas.DataFrame`. 
   The following columns must be there: 
   * ArticleText
   * System -- the name of the system/summarizer
   * SystemSummary -- a system summary generated by the `System` at the same row
   * ReferenceSummary -- corresponding reference summary provided by the dataset 
   * Various columns of human evaluation aspects, as defined in `env.human_metrics`
2. `batch_result_df`: Each row corresponds to a sample and each column corresponds to a MultiIndex `(approach, model, score_name)`. `score_name` is a variant of a `model`, for example, ROUGE-1 is a `score_name` for ROUGE which is a `model`. 
3. `corr_df`: The correlation coefficient dataframe, the last column which is like 


# Results 
* Newsroom: see [results/newsroom.md](./results/newsroom.md)
