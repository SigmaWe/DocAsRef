{
  "('pearsonr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'P')": 0.2047459189,
  "('pearsonr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'R')": -0.030897684,
  "('pearsonr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'F')": 0.0488112851,
  "('pearsonr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'P')": 0.1959784657,
  "('pearsonr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'R')": -0.0126155162,
  "('pearsonr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'F')": 0.0653252154,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.7103451907,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.677323492,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.6857210847,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.6344054749,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.6706700242,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.6007196052,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.684676533,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.6172369801,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.7058534108,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'js-2')": 0.677294328,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'mover_score')": 0.6136235142,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_recall_score')": 0.7474204606,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_precision_score')": 0.5862066205,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_f_score')": 0.7118686152,
  "('kendalltau', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'P')": 0.1372133033,
  "('kendalltau', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'R')": -0.0244518552,
  "('kendalltau', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'F')": 0.0256643439,
  "('kendalltau', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'P')": 0.1372133033,
  "('kendalltau', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'R')": -0.0187935747,
  "('kendalltau', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'F')": 0.0418308598,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.4977266067,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.4948974664,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.47307267,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.4823684166,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.5013640727,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.4282105885,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.4633727605,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.434273032,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.5114681452,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'js-2')": 0.496514118,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'mover_score')": 0.4059816292,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_recall_score')": 0.5215722176,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_precision_score')": 0.3845609957,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_f_score')": 0.4803476021,
  "('spearmanr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'P')": 0.2009559042,
  "('spearmanr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'R')": -0.0311853992,
  "('spearmanr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-entropy', 'F')": 0.0355178714,
  "('spearmanr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'P')": 0.1995097466,
  "('spearmanr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'R')": -0.0167778288,
  "('spearmanr', 'litepyramid_recall', 'new', 'bertscore-sentence-pagerank-cos-deberta-large-sum', 'F')": 0.0553200299,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.6772938251,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.6773478309,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.6566635764,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.6614280957,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.6854187107,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.5922645569,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.6390816599,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.6105425492,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.6910833281,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'js-2')": 0.6761477001,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'mover_score')": 0.5736985332,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_recall_score')": 0.6980320855,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_precision_score')": 0.5369865316,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_f_score')": 0.6542813167
}