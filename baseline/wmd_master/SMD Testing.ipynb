{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcab9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading spacy\n"
     ]
    }
   ],
   "source": [
    "## Framework for Sentence Mover's Distance\n",
    "\n",
    "import sys, nltk\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import math\n",
    "from wmd import WMD\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"loading spacy\")\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def tokenize_texts(inLines):\n",
    "\n",
    "\t# input: raw input text\n",
    "\t# output: a list of token IDs, where a id_doc=[[ref],[hyp]],\n",
    "\t#           ref/hyp=[sent1, sent2,...], and a sent=[wordID1, wordID2 ... ]\n",
    "\n",
    "\tid_docs = []\n",
    "\ttext_docs = []\n",
    "\n",
    "\tfor doc in inLines:\n",
    "\t\tid_doc = []\n",
    "\t\ttext_doc = []\n",
    "\n",
    "\t\tfor i in range(2):  # iterate over ref and hyp\n",
    "\t\t\ttext = doc.split('\\t')[i].strip()\n",
    "\t\t\tsent_list = [sent for sent in nltk.sent_tokenize(text)]\n",
    "\t\t\tif WORD_REP == \"glove\":\n",
    "\t\t\t\tIDs = [[nlp.vocab.strings[t.text.lower()] for t in nlp(sent) if t.text.isalpha() and t.text.lower() not in stop_words] for sent in sent_list]\n",
    "\t\t\tif WORD_REP == \"elmo\":\n",
    "\t\t\t\t# no word IDs, just use spacy ids, but without lower/stop words\n",
    "\t\t\t\t# IDs = [[nlp.vocab.strings[t.text] for t in nlp(sent)] for sent in sent_list]\n",
    "\t\t\t\tIDs = [[nlp.vocab.strings[t.text] for t in nlp(sent)] for sent in sent_list]\n",
    "\t\t\tid_list = [x for x in IDs if x != []]  # get rid of empty sents\n",
    "\t\t\ttext_list = [[token.text for token in nlp(x)] for x in sent_list if x != []]\n",
    "\n",
    "\t\t\tid_doc.append(id_list)\n",
    "\t\t\ttext_doc.append(text_list)\n",
    "\t\tid_docs.append(id_doc)\n",
    "\t\ttext_docs.append(text_doc)\n",
    "\treturn id_docs, text_docs\n",
    "\n",
    "\n",
    "def get_embeddings(id_doc, text_doc):\n",
    "\n",
    "\t# input: a ref/hyp pair, with each piece is a list of sentences and each sentence is a list of token IDs\n",
    "\t# output: IDs (the orig doc but updating IDs as needed) and rep_map (a dict mapping word ids to embeddings).\n",
    "\t#           if sent emb, add list of sent emb to end of ref and hyp\n",
    "\n",
    "\trep_map = {}\n",
    "\n",
    "\t# if adding new IDs, make sure they don't overlap with existing IDs\n",
    "\t# to get max, flatten the list of IDs\n",
    "\tnew_id = max(sum(sum(id_doc, []), [])) + 1\n",
    "\n",
    "\tsent_ids = [[], []]  # keep track of sentence IDs for rep and hyp. won't use this for wms\n",
    "\n",
    "\tfor i in range(2):\n",
    "\n",
    "\t\tfor sent_i in range(len(id_doc[i])):\n",
    "\t\t\tsent_emb = []\n",
    "\t\t\tword_emb_list = []  # list of a sentence's word embeddings\n",
    "\t\t\t# get word embeddings\n",
    "\t\t\tif WORD_REP == \"glove\":\n",
    "\t\t\t\tfor wordID in id_doc[i][sent_i]:\n",
    "\t\t\t\t\tword_emb = nlp.vocab.get_vector(wordID)\n",
    "\t\t\t\t\tword_emb_list.append(word_emb)\n",
    "\t\t\tif WORD_REP == \"elmo\":\n",
    "\t\t\t\tsent_vec = MODEL.embed_batch([text_doc[i][sent_i]])\n",
    "\t\t\t\tsent_vec = sent_vec[0]  # 1 elt in batch\n",
    "\t\t\t\tword_emb_list = np.average(sent_vec, axis=0)  # average layers to get word embs\n",
    "\t\t\t\t# remove stopwords from elmo\n",
    "\t\t\t\tkeep_inds = []\n",
    "\t\t\t\tfor word_i in range(len(text_doc[i][sent_i])):\n",
    "\t\t\t\t\tword = text_doc[i][sent_i][word_i]\n",
    "\t\t\t\t\t# if the lower-cased word is a stop word or not alphabetic, remove it from emb and id\n",
    "\t\t\t\t\tif (word.isalpha()) and (word.lower() not in stop_words):\n",
    "\t\t\t\t\t\tkeep_inds.append(word_i)\n",
    "\t\t\t\tword_emb_list = [word_emb_list[x] for x in range(len(text_doc[i][sent_i])) if x in keep_inds]\n",
    "\t\t\t\tid_doc[i][sent_i] = [id_doc[i][sent_i][x] for x in range(len(text_doc[i][sent_i])) if x in keep_inds]\n",
    "\t\t\t\tassert(len(word_emb_list) == len(id_doc[i][sent_i]))\n",
    "\n",
    "\t\t\t# add word embeddings to embedding dict\n",
    "\t\t\tif METRIC != \"sms\":\n",
    "\t\t\t\tfor w_ind in range(len(word_emb_list)):\n",
    "\t\t\t\t\t# if the word is not already in the embedding dict, add it\n",
    "\t\t\t\t\tw_id = id_doc[i][sent_i][w_ind]\n",
    "\t\t\t\t\tif w_id not in rep_map:\n",
    "\t\t\t\t\t\trep_map[w_id] = word_emb_list[w_ind]\n",
    "\t\t\t\t\t# for contextualized embeddings, replace word ID with a unique ID and add it to the embedding dict\n",
    "\t\t\t\t\telif WORD_REP != \"glove\":\n",
    "\t\t\t\t\t\trep_map[new_id] = word_emb_list[w_ind]\n",
    "\t\t\t\t\t\tid_doc[i][sent_i][w_ind] = new_id\n",
    "\t\t\t\t\t\tnew_id += 1\n",
    "\n",
    "\t\t\t# add sentence embeddings to embedding dict\n",
    "\t\t\tif (METRIC != \"wms\") and (len(word_emb_list) > 0):\n",
    "\t\t\t\tsent_emb = get_sent_embedding(word_emb_list)\n",
    "\t\t\t\t# add sentence embedding to the embedding dict\n",
    "\t\t\t\trep_map[new_id] = sent_emb\n",
    "\t\t\t\tsent_ids[i].append(new_id)\n",
    "\t\t\t\tnew_id += 1\n",
    "\n",
    "\t# add sentence IDs to ID list\n",
    "\tif METRIC != \"wms\":\n",
    "\t\tfor j in range(len(id_doc)):\n",
    "\t\t\tid_doc[j].append(sent_ids[j])\n",
    "\n",
    "\treturn id_doc, rep_map\n",
    "\n",
    "\n",
    "def get_sent_embedding(emb_list):\n",
    "\n",
    "\t# input: list of a sentence's word embeddings\n",
    "\t# output: the sentence's embedding\n",
    "\n",
    "\temb_array = np.array(emb_list)\n",
    "\tsent_emb = list(np.mean(emb_array, axis=0))\n",
    "\n",
    "\treturn sent_emb\n",
    "\n",
    "\n",
    "def get_weights(id_doc):\n",
    "\n",
    "\t# input: a ref/hyp pair, with each piece is a list of sentences and each sentence is a list of token IDs.\n",
    "\t#           if the metric is not wms, there is also an extra list of sentence ids for ref and hyp\n",
    "\t# output: 1. a ref/hyp pair of 1-d lists of all word and sentence IDs (where applicable)\n",
    "\t#           2. a ref/hyp pair of arrays of weights for each of those IDs\n",
    "\n",
    "\t# Note that we only need to output counts; these will be normalized by the sum of counts in the WMD code.\n",
    "\n",
    "\t# 2 1-d lists of all relevant embedding IDs\n",
    "\tid_lists = [[], []]\n",
    "\t# 2 arrays where an embedding's weight is at the same index as its ID in id_lists\n",
    "\td_weights = [np.array([], dtype=np.float32), np.array([], dtype=np.float32)]\n",
    "\n",
    "\tfor i in range(len(id_doc)):  # for ref/hyp\n",
    "\t\tif METRIC != \"wms\":\n",
    "\t\t\t# pop off sent ids so id_doc is back to word ids only\n",
    "\t\t\tsent_ids = id_doc[i].pop()\n",
    "\n",
    "\t\t# collapse to 1-d\n",
    "\t\twordIDs = sum(id_doc[i], [])\n",
    "\t\t# get dict that maps from ID to count\n",
    "\t\tcounts = Counter(wordIDs)\n",
    "\n",
    "\t\t# get word weights\n",
    "\t\tif METRIC != \"sms\":\n",
    "\t\t\tfor k in counts.keys():\n",
    "\t\t\t\tid_lists[i].append(k)\n",
    "\t\t\t\td_weights[i] = np.append(d_weights[i], counts[k])\n",
    "\n",
    "\t\t# get sentence weights\n",
    "\t\tif METRIC != \"wms\":\n",
    "\t\t\t# weight words by counts and give each sentence a weight equal to the number of words in the sentence\n",
    "\t\t\tid_lists[i] += sent_ids\n",
    "\t\t\t# make sure to check no empty ids\n",
    "\t\t\td_weights[i] = np.append(d_weights[i], np.array([float(len(x)) for x in id_doc[i] if x != []], dtype=np.float32))\n",
    "\n",
    "\treturn id_lists, d_weights\n",
    "\n",
    "\n",
    "def print_score(inLines, out_file, results_list):\n",
    "\n",
    "\t# input: raw text, the output file, and the results\n",
    "\t# output: scores will be written to output file\n",
    "\n",
    "\tof = open(out_file, 'w')\n",
    "\tof.write(\"Average: \" + str(np.mean(results_list)) + \"\\n\")\n",
    "\tof.write(\"ID\\tReference\\tHypothesis\\t\"+METRIC)\n",
    "\tfor i in range(len(inLines)):\n",
    "\t\t[ref_str, hyp_str] = inLines[i].split('\\t')[:2]\n",
    "\t\tof.write('\\n' + str(i) + '\\t' + ref_str + '\\t' + hyp_str.strip(\"\\n\"))\n",
    "\t\tof.write('\\t' + str(results_list[i]))\n",
    "\tof.write('\\n')\n",
    "\tof.close()\n",
    "\treturn \"Done!\"\n",
    "\n",
    "\n",
    "def calc_smd(input_f, output_f=\"\"):\n",
    "\tinF = open(input_f, 'r')\n",
    "\tinLines = inF.readlines()\n",
    "\tinF.close()\n",
    "\tprint(\"Found\", len(inLines), \"documents\")\n",
    "\ttoken_doc_list, text_doc_list = tokenize_texts(inLines)\n",
    "\tcount = 0\n",
    "\tresults_list = []\n",
    "\tfor doc_id in range(len(token_doc_list)):\n",
    "\t\tdoc = token_doc_list[doc_id]\n",
    "\t\ttext = text_doc_list[doc_id]\n",
    "\t\t# transform doc to ID list, both words and/or sentences. get ID dict that maps to emb\n",
    "\t\t[ref_ids, hyp_ids], rep_map = get_embeddings(doc, text)\n",
    "\t\t# get D values\n",
    "\t\t[ref_id_list, hyp_id_list], [ref_d, hyp_d] = get_weights([ref_ids, hyp_ids])\n",
    "\t\t# format doc as expected: {id: (id, ref_id_list, ref_d)}\n",
    "\t\tdoc_dict = {\"0\": (\"ref\", ref_id_list, ref_d), \"1\": (\"hyp\", hyp_id_list, hyp_d)}\n",
    "\t\tcalc = WMD(rep_map, doc_dict, vocabulary_min=1)\n",
    "\t\ttry:\n",
    "\t\t\tdist = calc.nearest_neighbors(str(0), k=1, early_stop=1)[0][1]  # how far is hyp from ref?\n",
    "\t\texcept:\n",
    "\t\t\tprint(doc, text)\n",
    "\t\tsim = math.exp(-dist)  # switch to similarity\n",
    "\t\tresults_list.append(sim)\n",
    "\t\tif doc_id == int((len(token_doc_list) / 10.) * count):\n",
    "\t\t\tprint(str(count * 10) + \"% done with calculations\")\n",
    "\t\t\tcount += 1\n",
    "\tif output_f != \"\":\n",
    "\t\tprint_score(inLines, output_f, results_list)\n",
    "\telse:\n",
    "\t\tprint(\"Results: \", np.mean(results_list))\n",
    "\n",
    "\treturn 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27afcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_REP = 'glove'\n",
    "METRIC = 'SMS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf913e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"With plenty at stake for both United and City -- both sides are challenging for a top-four spot in the Premier League -- some may feel that the 4pm start could be unwise as it allows supporters more time in the pub .   Police say they have no objections to the late afternoon kick-off for Sunday 's Manchester derby at Old Trafford .    However , the man in charge of patrolling the sell-out clash has no such fears .\",\n",
    " \"police say they have no objections to the late afternoon kick-off for sunday 's manchester derby at old trafford . police say they have no objections to sunday 's manchester derby taking place in the late afternoon .  rio ferdinand was hit by an object thrown from the crowd during the manchester derby in december 2012 .\",\n",
    " \"Police say they have no objections to the late afternoon kick-off for Sunday 's Manchester derby at Old Trafford . With plenty at stake for both United and City -- both sides are challenging for a top-four spot in the Premier League -- some may feel that the 4pm start could be unwise as it allows supporters more time in the pub .   However , the man in charge of patrolling the sell-out clash has no such fears .   Chief Superintendent John O'Hare says the kick-off was agreed by all parties and revealed that the decision is down to good behaviour from supporters of each side .\",\n",
    " \"police say they have no objections to the late afternoon kick-off for sunday 's manchester derby at old trafford . police say they have no objections to sunday 's manchester derby taking place in the late afternoon  however , the man in charge of patrolling the sell-out clash has no such fears .\",\n",
    " \"police say they have no objections to the late afternoon kick-off for sunday 's manchester derby at old trafford . police say they have no objections to sunday 's manchester derby taking place in the late afternoon .  however , the man in charge of patrolling the sell-out clash has no such fears .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9188b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [\"manchester united take on manchester city on sunday .    match will begin at 4pm local time at united 's old trafford home .    police have no objections to kick-off being so late in the afternoon .    last late afternoon weekend kick-off in the manchester derby saw 34 fans arrested at wembley in 2011 fa cup semi-final .\",\n",
    " \"manchester united take on manchester city on sunday .    match will begin at 4pm local time at united 's old trafford home .    police have no objections to kick-off being so late in the afternoon .    last late afternoon weekend kick-off in the manchester derby saw 34 fans arrested at wembley in 2011 fa cup semi-final .\",\n",
    " \"manchester united take on manchester city on sunday .    match will begin at 4pm local time at united 's old trafford home .    police have no objections to kick-off being so late in the afternoon .    last late afternoon weekend kick-off in the manchester derby saw 34 fans arrested at wembley in 2011 fa cup semi-final .\",\n",
    " \"manchester united take on manchester city on sunday .    match will begin at 4pm local time at united 's old trafford home .    police have no objections to kick-off being so late in the afternoon .    last late afternoon weekend kick-off in the manchester derby saw 34 fans arrested at wembley in 2011 fa cup semi-final .\",\n",
    " \"manchester united take on manchester city on sunday .    match will begin at 4pm local time at united 's old trafford home .    police have no objections to kick-off being so late in the afternoon .    last late afternoon weekend kick-off in the manchester derby saw 34 fans arrested at wembley in 2011 fa cup semi-final .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151c345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b673b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370a7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = []\n",
    "\n",
    "for r, p in zip(predictions, references):\n",
    "    inpt.append(str(r) + str('\\t') + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250d5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_doc_list, text_doc_list = tokenize_texts(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6ce10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for doc_id in range(len(token_doc_list)):\n",
    "    doc = token_doc_list[doc_id]\n",
    "    text = text_doc_list[doc_id]\n",
    "    # transform doc to ID list, both words and/or sentences. get ID dict that maps to emb\n",
    "    [ref_ids, hyp_ids], rep_map = get_embeddings(doc, text)\n",
    "    # get D values\n",
    "    [ref_id_list, hyp_id_list], [ref_d, hyp_d] = get_weights([ref_ids, hyp_ids])\n",
    "    # format doc as expected: {id: (id, ref_id_list, ref_d)}\n",
    "    doc_dict = {\"0\": (\"ref\", ref_id_list, ref_d), \"1\": (\"hyp\", hyp_id_list, hyp_d)}\n",
    "    calc = WMD(rep_map, doc_dict, vocabulary_min=1)\n",
    "    try:\n",
    "        dist = calc.nearest_neighbors(str(0), k=1, early_stop=1)[0][1]  # how far is hyp from ref?\n",
    "    except:\n",
    "        print(doc, text)\n",
    "    sim = math.exp(-dist)  # switch to similarity\n",
    "    results_list.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e3dbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.8343197425090665e-14,\n",
       " 1.2281843044283268e-13,\n",
       " 2.7411905441384962e-15,\n",
       " 9.066640457021707e-14,\n",
       " 5.761425746538699e-14]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b781c5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.8865690809484005, pvalue=0.045071313501270534)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "x = results_list\n",
    "y = [-14,-13,-15,-14,-14]\n",
    "scipy.stats.pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a1f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde6490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbead080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a32480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0232a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c37d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf0e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62108a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca8beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e5afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8df44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351ae89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2447a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
