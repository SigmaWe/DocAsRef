{
  "('pearsonr', 'litepyramid_recall', 'new', 'pagerank-cos-mpnet-entropy', 'score')": -0.2017422018,
  "('pearsonr', 'litepyramid_recall', 'new', 'pagerank-cos-mpnet-sum', 'score')": 0.0920938336,
  "('pearsonr', 'litepyramid_recall', 'new', 'pagerank-mnli-deberta-entail_contradict-entropy', 'score')": -0.0253524703,
  "('pearsonr', 'litepyramid_recall', 'new', 'pagerank-mnli-deberta-entail_contradict-sum', 'score')": -0.0363290802,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.7103451907,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.677323492,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.6857210847,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.6344054749,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.6706700242,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.6007196052,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.684676533,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.6172369801,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.7058534108,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'js-2')": 0.677294328,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'mover_score')": 0.6136235142,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_recall_score')": 0.7474204606,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_precision_score')": 0.5862066205,
  "('pearsonr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_f_score')": 0.7118686152,
  "('kendalltau', 'litepyramid_recall', 'new', 'pagerank-cos-mpnet-entropy', 'score')": -0.1389493654,
  "('kendalltau', 'litepyramid_recall', 'new', 'pagerank-cos-mpnet-sum', 'score')": 0.0639393015,
  "('kendalltau', 'litepyramid_recall', 'new', 'pagerank-mnli-deberta-entail_contradict-entropy', 'score')": 0.0147519457,
  "('kendalltau', 'litepyramid_recall', 'new', 'pagerank-mnli-deberta-entail_contradict-sum', 'score')": 0.0159644344,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.4977266067,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.4948974664,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.47307267,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.4823684166,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.5013640727,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.4282105885,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.4633727605,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.434273032,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.5114681452,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'js-2')": 0.496514118,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'mover_score')": 0.4059816292,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_recall_score')": 0.5215722176,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_precision_score')": 0.3845609957,
  "('kendalltau', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_f_score')": 0.4803476021,
  "('spearmanr', 'litepyramid_recall', 'new', 'pagerank-cos-mpnet-entropy', 'score')": -0.184752546,
  "('spearmanr', 'litepyramid_recall', 'new', 'pagerank-cos-mpnet-sum', 'score')": 0.0924045967,
  "('spearmanr', 'litepyramid_recall', 'new', 'pagerank-mnli-deberta-entail_contradict-entropy', 'score')": 0.0178399446,
  "('spearmanr', 'litepyramid_recall', 'new', 'pagerank-mnli-deberta-entail_contradict-sum', 'score')": 0.0186740355,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_f_score')": 0.6772938251,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_recall')": 0.6773478309,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_recall')": 0.6566635764,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_precision')": 0.6614280957,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_2_f_score')": 0.6854187107,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_precision')": 0.5922645569,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_1_recall')": 0.6390816599,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_precision')": 0.6105425492,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'rouge_l_f_score')": 0.6910833281,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'js-2')": 0.6761477001,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'mover_score')": 0.5736985332,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_recall_score')": 0.6980320855,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_precision_score')": 0.5369865316,
  "('spearmanr', 'litepyramid_recall', 'PreCalc', 'PreCalc', 'bert_f_score')": 0.6542813167
}